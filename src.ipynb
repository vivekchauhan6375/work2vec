{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.10/site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lemmatized_tweets (1).pkl\",\"rb\") as file_handle:\n",
    "    data1 = pickle.load(file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [twitpic, com, zl, awww, bummer, shoulda, get,...\n",
       "1          [upset, update, facebook, texte, cry, result, ...\n",
       "2                                [manage, save, rest, bound]\n",
       "3                            [body, feel, itchy, like, fire]\n",
       "4                                              [behave, mad]\n",
       "                                 ...                        \n",
       "1599995                           [wake, school, good, feel]\n",
       "1599996      [thewdb, com, cool, hear, old, walt, interview]\n",
       "1599997                 [ready, mojo, makeover, ask, detail]\n",
       "1599998    [happy, th, birthday, boo, alll, time, tupac, ...\n",
       "1599999    [happy, charitytuesday, thenspcc, sparkscharit...\n",
       "Name: text, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_raw_text = list(data[\"text\"].apply(lambda x: \" \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_raw_text = list(filter(lambda x: len(x) > 1,converted_raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100000 tweets have been saved to 'first_100000_tweets.txt'.\n"
     ]
    }
   ],
   "source": [
    "first_100000_tweets = converted_raw_text[:100000]\n",
    "\n",
    "# Save to a text file\n",
    "with open('first_100000_tweets.txt', 'w', encoding='utf-8') as file:\n",
    "    for tweet in first_100000_tweets:\n",
    "        file.write(tweet + '\\n')\n",
    "\n",
    "print(\"First 100000 tweets have been saved to 'first_100000_tweets.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100000 examples [00:00, 2947031.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dset = datasets.load_dataset('text', data_files={\"train\":'first_100000_tweets.txt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(single_row):\n",
    "    single_row['tokenized-tweets'] = single_row[\"text\"].split()\n",
    "    return single_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [00:03<00:00, 32822.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dset = dset.map(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset['train']['tokenized-tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ngrams in ./.venv/lib/python3.10/site-packages (1.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def convert_to_trigrams(single_row):\n",
    "    single_row[\"tri-grams\"] = list(ngrams(single_row['tokenized-tweets'],n=3))\n",
    "    return single_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [00:05<00:00, 18296.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dset['train'] = dset['train'].map(convert_to_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'tokenized-tweets', 'tri-grams'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twitpic com zl awww bummer shoulda get david carr day'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_raw_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for raw_text in converted_raw_text[:100000]:\n",
    "    vocabulary.update(raw_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43953"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idx = dict(zip(vocabulary, range(len(vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bigrams(single_row):\n",
    "    center_token_token_pairs = list()\n",
    "    for single_trigrams in single_row[\"tri-grams\"]:\n",
    "        bigrams = list()\n",
    "        bigrams.append([vocab2idx[single_trigrams[1]], vocab2idx[single_trigrams[0]]])\n",
    "        bigrams.append([vocab2idx[single_trigrams[1]], vocab2idx[single_trigrams[2]]])\n",
    "        center_token_token_pairs.append(bigrams)\n",
    "    single_row['tri-grams'] = center_token_token_pairs\n",
    "    \n",
    "    return single_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [00:10<00:00, 9793.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dset['train'] = dset['train'].map(convert_to_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'twitpic com zl awww bummer shoulda get david carr day',\n",
       " 'tokenized-tweets': ['twitpic',\n",
       "  'com',\n",
       "  'zl',\n",
       "  'awww',\n",
       "  'bummer',\n",
       "  'shoulda',\n",
       "  'get',\n",
       "  'david',\n",
       "  'carr',\n",
       "  'day'],\n",
       " 'tri-grams': [[[9652, 5980], [9652, 22010]],\n",
       "  [[22010, 9652], [22010, 23609]],\n",
       "  [[23609, 22010], [23609, 9043]],\n",
       "  [[9043, 23609], [9043, 33850]],\n",
       "  [[33850, 9043], [33850, 20179]],\n",
       "  [[20179, 33850], [20179, 31273]],\n",
       "  [[31273, 20179], [31273, 14319]],\n",
       "  [[14319, 31273], [14319, 19647]]]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_target_token_pairs = list()\n",
    "for single_tweet_bigrams in dset['train']['tri-grams']:\n",
    "    for bigrams_list in single_tweet_bigrams:\n",
    "        input_token_target_token_pairs.append(bigrams_list[0])\n",
    "        input_token_target_token_pairs.append(bigrams_list[1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9652, 5980], [9652, 22010], [22010, 9652], [22010, 23609]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_target_token_pairs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
